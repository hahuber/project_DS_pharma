# digital_twin/data_analyzer.py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def load_data(file_path='simulation_results.csv'):
    df = pd.read_csv(file_path)
    return df

def explore_final_status_distribution(df):
    plt.figure(figsize=(8, 6))
    sns.countplot(x='final_status', data=df)
    plt.title("Final Production Status Distribution")
    plt.xlabel("Final Status")
    plt.ylabel("Count")
    plt.show()

def correlation_analysis(df):
    # Select only numeric columns for correlation analysis
    numeric_cols = ['mixing_time', 'mixing_speed', 'uniformity_index', 'granulation_time',
                    'binder_rate', 'granule_density', 'drying_temp', 'moisture_content',
                    'comp_pressure', 'tablet_hardness', 'weight_variation', 'dissolution', 'yield_percent']
    corr_matrix = df[numeric_cols].corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title("Correlation Matrix of Process Parameters")
    plt.show()

def main():
    df = load_data()
    explore_final_status_distribution(df)
    correlation_analysis(df)
    
if __name__ == "__main__":
    main()
# digital_twin/data_collector.py

import json
from sqlalchemy.orm import sessionmaker
from digital_twin.data_storage import engine, SimulationResult
from data_generation.simulator import run_simulation

SessionLocal = sessionmaker(bind=engine)
session = SessionLocal()

def collect_data(num_simulations=20000):
    """
    Runs the simulation multiple times and saves results to the database.
    Updated to capture rewards and logs from each stage.
    """
    for i in range(num_simulations):
        results = run_simulation()

        # Extract fields from each stage
        sim_entry = SimulationResult(
            # Mixing stage
            mixing_status=results.get('mixing', {}).get('status'),
            mixing_operator=results.get('mixing', {}).get('operator'),
            mixing_time=results.get('mixing', {}).get('mixing_time'),
            mixing_speed=results.get('mixing', {}).get('mixing_speed'),
            uniformity_index=results.get('mixing', {}).get('uniformity_index'),
            mixing_reward=results.get('mixing', {}).get('reward'),
            mixing_log=json.dumps(results.get('mixing', {}).get('log')),
            
            # Granulation stage
            granulation_status=results.get('granulation', {}).get('status'),
            granulation_operator=results.get('granulation', {}).get('operator'),
            granulation_time=results.get('granulation', {}).get('granulation_time'),
            binder_rate=results.get('granulation', {}).get('binder_rate'),
            granule_density=results.get('granulation', {}).get('granule_density'),
            granulation_reward=results.get('granulation', {}).get('reward'),
            granulation_log=json.dumps(results.get('granulation', {}).get('log')),
            
            # Drying stage
            drying_status=results.get('drying', {}).get('status'),
            drying_operator=results.get('drying', {}).get('operator'),
            drying_temp=results.get('drying', {}).get('drying_temp'),
            moisture_content=results.get('drying', {}).get('moisture_content'),
            drying_reward=results.get('drying', {}).get('reward'),
            drying_log=json.dumps(results.get('drying', {}).get('log')),
            
            # Compression stage
            compression_status=results.get('compression', {}).get('status'),
            compression_operator=results.get('compression', {}).get('operator'),
            comp_pressure=results.get('compression', {}).get('comp_pressure'),
            tablet_hardness=results.get('compression', {}).get('tablet_hardness'),
            weight_variation=results.get('compression', {}).get('weight_variation'),
            dissolution=results.get('compression', {}).get('dissolution'),
            yield_percent=results.get('compression', {}).get('yield_percent'),
            compression_reward=results.get('compression', {}).get('reward'),
            compression_log=json.dumps(results.get('compression', {}).get('log')),
            
            # Overall simulation result
            final_status=results.get('final_status'),
            failure_reason=(results.get('mixing', {}).get('reason') or
                            results.get('granulation', {}).get('reason') or
                            results.get('drying', {}).get('reason') or
                            results.get('compression', {}).get('reason') or '')
        )

        session.add(sim_entry)
        session.commit()

        print(f"Simulation {i+1}/{num_simulations} stored in DB.")

    session.close()

if __name__ == "__main__":
    collect_data()
# digital_twin/data_storage.py

import json
from sqlalchemy import create_engine, Column, Integer, Float, String, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# Define database file
DATABASE_FILE = "digital_twin/simulation_data.db"

# Initialize SQLAlchemy
Base = declarative_base()
engine = create_engine(f"sqlite:///{DATABASE_FILE}")
SessionLocal = sessionmaker(bind=engine)

# Define a table to store simulation results with additional fields for rewards and logs.
class SimulationResult(Base):
    __tablename__ = "simulation_runs"

    id = Column(Integer, primary_key=True, autoincrement=True)
    
    # Mixing stage fields
    mixing_status = Column(String)
    mixing_operator = Column(String)
    mixing_time = Column(Float)
    mixing_speed = Column(Float)
    uniformity_index = Column(Float)
    mixing_reward = Column(Float)
    mixing_log = Column(Text)  # JSON serialized

    # Granulation stage fields
    granulation_status = Column(String)
    granulation_operator = Column(String)
    granulation_time = Column(Float)
    binder_rate = Column(Float)
    granule_density = Column(Float)
    granulation_reward = Column(Float)
    granulation_log = Column(Text)  # JSON serialized

    # Drying stage fields
    drying_status = Column(String)
    drying_operator = Column(String)
    drying_temp = Column(Float)
    moisture_content = Column(Float)
    drying_reward = Column(Float)
    drying_log = Column(Text)  # JSON serialized

    # Compression stage fields
    compression_status = Column(String)
    compression_operator = Column(String)
    comp_pressure = Column(Float)
    tablet_hardness = Column(Float)
    weight_variation = Column(Float)
    dissolution = Column(Float)
    yield_percent = Column(Float)
    compression_reward = Column(Float)
    compression_log = Column(Text)  # JSON serialized

    # Overall simulation outcome
    final_status = Column(String)
    failure_reason = Column(String)

# Create tables
def init_db():
    Base.metadata.create_all(engine)

if __name__ == "__main__":
    init_db()
    print("Database initialized successfully.")
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from sqlalchemy.orm import sessionmaker
from digital_twin.data_storage import engine, SimulationResult
from sklearn.preprocessing import StandardScaler

class ProductionDataset(Dataset):
    """
    PyTorch Dataset that loads simulation data from the database.
    The features are the measurable process parameters (excluding yield_percent),
    and the target is the yield_percent.
    """
    def __init__(self):
        # Load data from the database
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()
        results = session.query(SimulationResult).all()
        session.close()

        if not results:
            print("No simulation data found in the database. Please run the data collector first.")
            sys.exit(1)

        # Define the input features that are realistically measurable.
        # We exclude 'yield_percent' since that is our prediction target.
        self.features = np.array([
            [r.mixing_time, r.mixing_speed, r.uniformity_index, r.granulation_time,
             r.binder_rate, r.granule_density, r.drying_temp, r.moisture_content,
             r.comp_pressure, r.tablet_hardness, r.weight_variation, r.dissolution]
            for r in results
        ])

        # The target is the yield_percent value.
        self.labels = np.array([r.yield_percent for r in results])

        # Standardize features to help the training process.
        self.scaler = StandardScaler()
        self.features = self.scaler.fit_transform(self.features)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        # Return features and label as PyTorch tensors.
        x = torch.tensor(self.features[idx], dtype=torch.float32)
        y = torch.tensor(self.labels[idx], dtype=torch.float32)
        return x, y

def evaluate_model(model, loader, criterion):
    """
    Evaluate the model on a given DataLoader.
    Returns the average loss over the dataset.
    """
    model.eval()
    total_loss = 0.0
    count = 0
    with torch.no_grad():
        for features, targets in loader:
            outputs = model(features).squeeze()
            loss = criterion(outputs, targets)
            total_loss += loss.item() * features.size(0)
            count += features.size(0)
    return total_loss / count

def train_model():
    # Load the dataset
    dataset = ProductionDataset()
    total_size = len(dataset)
    print(f"Total simulation records: {total_size}")

    # Split the dataset: 80% training, 20% validation.
    train_size = int(0.8 * total_size)
    val_size = total_size - train_size
    train_set, val_set = random_split(dataset, [train_size, val_size])
    print(f"Training set size: {len(train_set)}, Validation set size: {len(val_set)}")

    # Create DataLoaders
    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_set, batch_size=32, shuffle=False)

    # Define the regression model.
    # Input dimension is 12 (the number of realistic features), and the output is a single continuous value.
    model = nn.Sequential(
        nn.Linear(12, 64),
        nn.ReLU(),
        nn.Linear(64, 1)  # No activation function, since we predict a continuous value.
    )

    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()  # For regression tasks

    num_epochs = 50
    for epoch in range(num_epochs):
        model.train()
        total_train_loss = 0.0

        for features, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(features).squeeze()
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item() * features.size(0)

        avg_train_loss = total_train_loss / train_size
        val_loss = evaluate_model(model, val_loader, criterion)
        print(f"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}")

    # Save the trained model
    torch.save(model.state_dict(), "digital_twin/production_model.pth")
    print("Model training complete and saved as 'digital_twin/production_model.pth'.")

if __name__ == "__main__":

    train_model()
# digital_twin/query_data.py

from sqlalchemy.orm import sessionmaker
from digital_twin.data_storage import engine, SimulationResult
import json

SessionLocal = sessionmaker(bind=engine)
session = SessionLocal()

def query_simulations(limit=10):
    """
    Query the first `limit` simulation records and print key details.
    """
    results = session.query(SimulationResult).limit(limit).all()
    for result in results:
        print(f"Run ID: {result.id}")
        print(f"  Final Status: {result.final_status}")
        print(f"  Mixing: status={result.mixing_status}, reward={result.mixing_reward}")
        print(f"  Granulation: status={result.granulation_status}, reward={result.granulation_reward}")
        print(f"  Drying: status={result.drying_status}, reward={result.drying_reward}")
        print(f"  Compression: status={result.compression_status}, reward={result.compression_reward}")
        print(f"  Failure Reason: {result.failure_reason}")
        print("  ---")
        # Optionally, deserialize logs:
        mixing_log = json.loads(result.mixing_log) if result.mixing_log else {}
        print(f"  Mixing Log: {mixing_log}")
    session.close()

if __name__ == "__main__":
    query_simulations()
